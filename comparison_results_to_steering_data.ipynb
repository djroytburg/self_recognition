{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Results and Export to Vector Steering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/droytbu/.hackenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--f=/home/droytbu/.local/share/jupyter/runtime/kernel-v35b9613dbe5fa5ea9ec2e1a4e7eb552192876d7ce.json\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from data import SOURCES, load_data\n",
    "from prompts import *\n",
    "TARGET = \"llama3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CNN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"individual_setting/score_results/cnn/llama3.1-8b-instruct_comparison_results_extra.json\",\"r\"))\n",
    "responses, articles, keys = load_data(\"cnn\", sources= ['gpt35', TARGET], extras=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambivalence check\n",
    "\n",
    "Ambivalence is when a model does not prefer the same result when queried twice in two different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0 \n",
    "ambivalent = []\n",
    "for result in data:\n",
    "    if result['model'] == 'gpt35':\n",
    "        total += 1\n",
    "        if result['backward_comparison'] != result['forward_comparison']:\n",
    "            ambivalent.append(result)\n",
    "len(ambivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6271477130325565"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a[1] for a in ambivalent) / len(ambivalent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Function\n",
    "\n",
    "Very important -- allows reconstruction of initial prompts to generate steering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(result, responses, articles, source='llama3.1-8b-instruct', forward=True):\n",
    "    article = articles[result['key']]\n",
    "    source_summary = responses[source][result['key']]\n",
    "    other_summary = responses[result['model']][result['key']]\n",
    "    if forward:\n",
    "        return COMPARISON_PROMPT_TEMPLATE.format(summary1=source_summary, summary2=other_summary, article=article)\n",
    "    else:\n",
    "        return COMPARISON_PROMPT_TEMPLATE.format(summary1=other_summary, summary2=source_summary, article=article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter criterion:\n",
    "\n",
    "1. **Positive case** (model selects *2* when backwards and *1* when forwards) or **Negative case** (model selects *1* when backwards and *2* when forwards), no ambivalent answers.\n",
    "2. **Threshold** (model selects *1* when backwards and *2* when forwards): averaging confidence values should be greater than parameterized thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1071 964 107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7176318706374134"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meets_criteria = 0\n",
    "t_pos, t_neg = 0.7, 0.7\n",
    "total = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "total_neg_conf = 0\n",
    "total_pos_conf = 0\n",
    "pos_samples = []\n",
    "neg_samples = []\n",
    "for result in data:\n",
    "    if result['model'] == 'gpt35':\n",
    "        total += 1\n",
    "        if result['backward_comparison'] == '2' and result['forward_comparison'] == '1':\n",
    "            pos_conf = 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "            if result['forward_comparison_probability'] > t_pos and result['backward_comparison_probability'] > t_pos:\n",
    "                meets_criteria += 1\n",
    "                pos += 1\n",
    "                total_pos_conf += 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "                result['forward_prompt'] = reconstruct(result, responses, articles)\n",
    "                result['backward_prompt'] = reconstruct(result, responses, articles, forward=False)\n",
    "                pos_samples.append(result)\n",
    "                pos_samples.append(reconstruct(result, responses, articles, forward=False))\n",
    "        if result['forward_comparison'] == '2' and result['backward_comparison'] == '1':\n",
    "            neg_conf = 0.5 * (result['forward_comparison_probability'] + result['backward_comparison_probability'])\n",
    "            if neg_conf > t_neg:\n",
    "                meets_criteria += 1\n",
    "                neg += 1\n",
    "                total_neg_conf += neg_conf\n",
    "                result['forward_prompt'] = reconstruct(result, responses, articles)\n",
    "                result['backward_prompt'] = reconstruct(result, responses, articles, forward=False)\n",
    "                neg_samples.append(result)\n",
    "print(meets_criteria, pos, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump({\"pos\": pos_samples, \"neg\": neg_samples}, open(\"vector_steering_samples.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hackenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
